{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def generate_checkerboard_samples(N, size):\n",
    "    # Initialize a tensor to hold the N samples of checkerboard patterns\n",
    "    samples = torch.zeros((N, size, size, 3))  # 3 for RGB channels\n",
    "\n",
    "    for i in range(N):\n",
    "        # Generate random colors for the checkerboard\n",
    "        color1 = torch.rand(3, dtype=torch.float32)  # Color for one set of squares\n",
    "        color2 = torch.rand(3, dtype=torch.float32)  # Color for the other set of squares\n",
    "\n",
    "        for x in range(size):\n",
    "            for y in range(size):\n",
    "                if (x + y) % 2 == 0:\n",
    "                    samples[i, x, y] = color1\n",
    "                else:\n",
    "                    samples[i, x, y] = color2\n",
    "\n",
    "    return samples\n",
    "\n",
    "def visualize(matrix : np.ndarray, title : str = \"\"):\n",
    "    if matrix.ndim == 2:  # Grayscale image\n",
    "        plt.imshow(matrix, cmap='gray', interpolation='none', vmin=0, vmax=1)\n",
    "    else:  # RGB image\n",
    "        plt.imshow(matrix, interpolation='none', vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def do_noise_skip(input : torch.tensor, t : int, beta_t : float):\n",
    "    ret = copy.deepcopy(input)\n",
    "\n",
    "    for i in range(t):\n",
    "        ret = ret * np.sqrt(1 - beta_t) + torch.randn(input.shape) * np.sqrt(beta_t)\n",
    "\n",
    "    # ret = torch.clamp(ret, 0, 1) # do they do this in the original paper?\n",
    "    return ret\n",
    "\n",
    "def do_fast_noise_skip(input : torch.tensor, t : int, beta_t : float):\n",
    "    # might be able to get further optimized\n",
    "\n",
    "    alpha_t_cum = 1\n",
    "    for i in range(1, t+1):\n",
    "        alpha_t_cum *= (1 - beta_t)\n",
    "\n",
    "    standard_normal = torch.randn(input.shape)\n",
    "    noise = standard_normal * np.sqrt(1- alpha_t_cum)\n",
    "\n",
    "    input_with_noise = input + noise\n",
    "\n",
    "    # input_with_noise = torch.clamp(input_with_noise, 0, 1) # do they do this in the original paper?\n",
    "\n",
    "    return input_with_noise, standard_normal\n",
    "\n",
    "def get_MNIST(grid_size) -> torch.tensor:\n",
    "    # returns inputs, images of digits\n",
    "\n",
    "    class FlattenTransform:\n",
    "        def __call__(self, tensor):\n",
    "            return tensor.view(-1)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((grid_size, grid_size)),\n",
    "    transforms.ToTensor(),\n",
    "    FlattenTransform(),\n",
    "    ])\n",
    "\n",
    "    mnist_train = datasets.MNIST('mnist_data', train=True, download=True, transform=transform)\n",
    "\n",
    "    digit_1_indices = [i for i, label in enumerate(mnist_train.targets) if label == 1]\n",
    "    digit_1_images = torch.stack([mnist_train[i][0] for i in digit_1_indices])\n",
    "\n",
    "    return digit_1_images\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    # big part of this function can be vectorized\n",
    "    def __init__(self, nr_noised_img_per_img_without_noise, nr_unique_img_without_noise, beta_t, T, USE_EXISTING_DATASET=False, grid_size=None, nr_channels=None):\n",
    "        self.USE_EXISTING_DATASET = USE_EXISTING_DATASET\n",
    "        self.grid_size = grid_size\n",
    "        self.beta_t = beta_t\n",
    "        self.nr_channels = nr_channels\n",
    "        self.nr_noised_img_per_img_without_noise = nr_noised_img_per_img_without_noise\n",
    "        self.nr_unique_img_without_noise = nr_unique_img_without_noise\n",
    "\n",
    "        self.nr_samples = nr_unique_img_without_noise * nr_noised_img_per_img_without_noise\n",
    "\n",
    "        if not USE_EXISTING_DATASET:\n",
    "            self.__without_noise = generate_checkerboard_samples(self.nr_samples, self.grid_size)\n",
    "        else:\n",
    "            assert nr_channels == None\n",
    "            self.__without_noise = get_MNIST(self.grid_size)\n",
    "            self.nr_channels = 1\n",
    "            assert self.nr_unique_img_without_noise <= len(self.__without_noise)\n",
    "\n",
    "        with_noise_shape = list(self.__without_noise.shape)\n",
    "        with_noise_shape[0] = self.nr_samples\n",
    "        self.with_noise = torch.empty(with_noise_shape)\n",
    "\n",
    "        self.standard_normal = torch.empty(self.with_noise.shape)\n",
    "        self.t_arr = torch.empty(self.nr_samples, dtype=torch.int)\n",
    "\n",
    "        for i in range(self.nr_samples):\n",
    "            self.t_arr[i] = np.random.randint(low=1, high=T) if T >= 2 else 1\n",
    "            self.with_noise[i], self.standard_normal[i] = do_fast_noise_skip(self.__without_noise[i//nr_noised_img_per_img_without_noise], self.t_arr[i], beta_t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.with_noise[idx].view(-1), self.standard_normal[idx].view(-1)\n",
    "        # return self.with_noise[idx].view(-1), self.without_noise[idx].view(-1)\n",
    "        # is it non optimal to call view here every time?\n",
    "    \n",
    "    def get_original_img(self, dataset_idx : int):\n",
    "        assert dataset_idx < self.nr_samples\n",
    "        return self.__without_noise[dataset_idx//self.nr_noised_img_per_img_without_noise]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.with_noise)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "T = 400\n",
    "my_dataset = MyDataset(USE_EXISTING_DATASET=True, T=T, beta_t=0.03, nr_unique_img_without_noise=100, nr_noised_img_per_img_without_noise=100, grid_size=16)\n",
    "my_dataloader = DataLoader(my_dataset, batch_size=len(my_dataset), shuffle=True)\n",
    "\n",
    "# assert len(my_dataset) == len(my_dataloader)\n",
    "\n",
    "print(len(my_dataset), len(my_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        gs = my_dataset.grid_size\n",
    "        nc = my_dataset.nr_channels\n",
    "        \n",
    "        nr_hidden = 3*nc if not my_dataset.USE_EXISTING_DATASET else 800\n",
    "        self.fc1 = nn.Linear(in_features=gs*gs*nc, out_features=nr_hidden)\n",
    "\n",
    "        self.sigmoid1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=nr_hidden, out_features=gs*gs*nc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = self.sigmoid1(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        output += x\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = DiffusionModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "cost_arr = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for with_noise, standard_normal in my_dataloader:\n",
    "        # TODO: put all data on GPU at once\n",
    "        with_noise, standard_normal = with_noise.to(device), standard_normal.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(with_noise)\n",
    "\n",
    "        loss = criterion(outputs, standard_normal)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], cost={running_loss}')\n",
    "\n",
    "    cost_arr.append(loss.item())\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "plt.plot(cost_arr, label='their loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_arr[int(len(cost_arr)*0.9):-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_t(t : int):\n",
    "    return my_dataset.beta_t # temp\n",
    "\n",
    "def do_inference(noised_input : torch.tensor, T, model, return_denoise_intermediates=False):\n",
    "\n",
    "    img = copy.deepcopy(noised_input)\n",
    "\n",
    "    if return_denoise_intermediates:\n",
    "        intermediates_arr = []\n",
    "        intermediates_arr.append(copy.deepcopy(img))\n",
    "\n",
    "    alpha_t_cum = 1\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(T):\n",
    "            alpha_t = 1 - get_beta_t(t)\n",
    "            alpha_t_cum *= alpha_t\n",
    "\n",
    "            c = (1 - alpha_t) / np.sqrt(1 - alpha_t_cum)\n",
    "            # random = torch.randn(img.shape) * get_beta_t(t)/20 # temp\n",
    "            random = 0 # temp\n",
    "            img = 1/np.sqrt(alpha_t) * (img - model(img) * c) + random\n",
    "\n",
    "            if return_denoise_intermediates:\n",
    "                intermediates_arr.append(copy.deepcopy(img))\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    if return_denoise_intermediates is False:\n",
    "        return img\n",
    "    else:\n",
    "        return img, intermediates_arr\n",
    "\n",
    "def visualize_inference(dataset, model, T, sample_indices, return_intermediates_arr=False):\n",
    "    if sample_indices == -1:\n",
    "        sample_indices = [i for i in range(len(dataset))]\n",
    "\n",
    "    if return_intermediates_arr:\n",
    "        intermediates_arr = []\n",
    "\n",
    "    for i in sample_indices:\n",
    "        print(i)\n",
    "\n",
    "        input_with_noise, standard_gausian = my_dataset[i]\n",
    "        original_img = my_dataset.get_original_img(i)\n",
    "        time_step = my_dataset.t_arr[i]\n",
    "\n",
    "        # Perform inference\n",
    "        inferred_img, intermediates = do_inference(input_with_noise, T, model, return_denoise_intermediates=return_intermediates_arr)\n",
    "\n",
    "        # Plot the images\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "        gs, nc = my_dataset.grid_size, my_dataset.nr_channels\n",
    "        \n",
    "        axes[0].imshow(original_img.view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(standard_gausian.view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "        axes[1].set_title('standard gausian used to generate noise')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(intermediates[-1].view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "        axes[2].set_title(f'final product')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        axes[3].imshow(input_with_noise.view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "        axes[3].set_title(f'Noised Image (t={time_step})')\n",
    "        axes[3].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if T < 10: # dont print if there is too many frames\n",
    "            fig, axes = plt.subplots(1, len(intermediates), figsize=(15, 5))\n",
    "            for i, denoised_intermediate in enumerate(intermediates):\n",
    "                axes[i].imshow(denoised_intermediate.view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "\n",
    "                axes[i].set_title(r'$x_{'+str(T-i)+r'}$')\n",
    "\n",
    "                axes[i].axis('off')\n",
    "\n",
    "            for i in range(4):\n",
    "                print()\n",
    "\n",
    "        if return_intermediates_arr:\n",
    "            intermediates_arr.append(intermediates)\n",
    "\n",
    "    if return_intermediates_arr:\n",
    "        return intermediates_arr\n",
    "\n",
    "intermediates_arr = visualize_inference(my_dataset, model, T=T, sample_indices=[0, 200, 900], return_intermediates_arr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\animation.py:892: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you output the Animation using `plt.show()` or `anim.save()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def tensors_to_gif(tensor_list, gif_path, total_duration, grid_size=16, nr_channels=1):\n",
    "    images = []\n",
    "\n",
    "    for tensor in tensor_list:\n",
    "        # Ensure tensor is detached and converted to CPU\n",
    "        tensor = tensor.view(grid_size, grid_size, nr_channels)\n",
    "        tensor = tensor.detach().cpu()\n",
    "\n",
    "        # Convert to numpy array\n",
    "        image_array = tensor.numpy()\n",
    "\n",
    "        # # Scale the image array to [0, 255] if needed\n",
    "        # if image_array.min() < 0 or image_array.max() > 1:\n",
    "        #     image_array = (image_array - image_array.min()) / (image_array.max() - image_array.min())\n",
    "\n",
    "        # Convert to 8-bit grayscale\n",
    "        image_array = (image_array * 255).astype(np.uint8)\n",
    "\n",
    "        # If the image is single-channel, remove the last dimension\n",
    "        if nr_channels == 1:\n",
    "            image_array = image_array.squeeze(-1)\n",
    "\n",
    "        # Add to list of images\n",
    "        images.append(image_array)\n",
    "        \n",
    "        # plt.imshow(image_array, interpolation='none')\n",
    "        # plt.show()\n",
    "\n",
    "    nr_frames = len(tensor_list)\n",
    "    print(nr_frames, T)\n",
    "    imageio.mimsave(gif_path, images, duration=total_duration/nr_frames)\n",
    "\n",
    "# Example usage\n",
    "# Assuming intermediates_arr[1] is a list of tensors\n",
    "# Adjust grid_size and nr_channels as per your dataset\n",
    "tensors_to_gif(intermediates_arr[1], 'output.gif', total_duration=1, grid_size=16, nr_channels=1)\n",
    "\n",
    "gif_path = 'output.gif'\n",
    "gif = imageio.mimread(gif_path)\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Function to update the frame\n",
    "def update_frame(frame):\n",
    "    plt.imshow(frame, interpolation='none', cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Create an animation\n",
    "ani = animation.FuncAnimation(fig, update_frame, frames=gif, repeat=True)\n",
    "\n",
    "# Display the animation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Axes' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_intermediates_arr:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m intermediates_arr\n\u001b[1;32m---> 44\u001b[0m intermediates_arr \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_inference_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_intermediates_arr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 19\u001b[0m, in \u001b[0;36mvisualize_inference_new\u001b[1;34m(dataset, model, T, return_intermediates_arr)\u001b[0m\n\u001b[0;32m     15\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     17\u001b[0m gs, nc \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mgrid_size, dataset\u001b[38;5;241m.\u001b[39mnr_channels\n\u001b[1;32m---> 19\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(intermediates[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(gs, gs, nc), vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m axes[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal product\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m axes[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Axes' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_inference_new(dataset, model, T, return_intermediates_arr=False):\n",
    "\n",
    "    if return_intermediates_arr:\n",
    "        intermediates_arr = []\n",
    "\n",
    "    gs = dataset.grid_size\n",
    "    nc = dataset.nr_channels\n",
    "\n",
    "    input_with_noise = torch.randn((gs, gs, nc)).view(-1)\n",
    "\n",
    "    # Perform inference\n",
    "    inferred_img, intermediates = do_inference(input_with_noise, T, model, return_denoise_intermediates=return_intermediates_arr)\n",
    "\n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 5))\n",
    "\n",
    "    gs, nc = dataset.grid_size, dataset.nr_channels\n",
    "\n",
    "    axes[2].imshow(intermediates[-1].view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "    axes[2].set_title(f'final product')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if T < 10: # dont print if there is too many frames\n",
    "        fig, axes = plt.subplots(1, len(intermediates), figsize=(15, 5))\n",
    "        for i, denoised_intermediate in enumerate(intermediates):\n",
    "            axes[i].imshow(denoised_intermediate.view(gs, gs, nc), vmin=0, vmax=1, interpolation='none')\n",
    "\n",
    "            axes[i].set_title(r'$x_{'+str(T-i)+r'}$')\n",
    "\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        for i in range(4):\n",
    "            print()\n",
    "\n",
    "    if return_intermediates_arr:\n",
    "        intermediates_arr.append(intermediates)\n",
    "\n",
    "    if return_intermediates_arr:\n",
    "        return intermediates_arr\n",
    "\n",
    "intermediates_arr = visualize_inference_new(dataset=my_dataset, model=model, T=T, return_intermediates_arr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
